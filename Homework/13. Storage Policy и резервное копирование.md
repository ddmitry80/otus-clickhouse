# Storage Policy и резервное копирование

## Настраиваем Minio

Изменения отражены в конфигурации кластера `sharded-cluster` текущего репозитория
- Добавлен контейнер с Minio в `docker-compose.yml`
- создано описание Storage Policy `configs/clickhouse-policy.json`
- Набор команд для minio cli, создающие пользователя для CH, бакет, storage policy, и применяющих эту политику для пользователя, описаны в `README.md` кластера

## Ставим clickhouse-backup на сервер

За основу экспериментов возьмем официальную документация, а именно https://kb.altinity.com/altinity-kb-setup-and-maintenance/clickhouse-backup/

Создадим `Dockerfile`, в котором опишем команды установки в контейнер

```Dockerfile
FROM clickhouse/clickhouse-server:25.1

# install clickhouse-backup
RUN mkdir clickhouse-backup && cd clickhouse-backup &&\
    wget https://github.com/Altinity/clickhouse-backup/releases/download/v2.6.8/clickhouse-backup_2.6.8_amd64.deb && \
    dpkg -i clickhouse-backup_2.6.8_amd64.deb && \
    rm clickhouse-backup_2.6.8_amd64.deb
```

Файл с настройками `configs/clickhouse-backup.yml` подключим в `/etc/clickhouse-backup/config.yml` на всех нодах через `docker-compose.yml`

Исходные данные
```sql
-- Загрузка исходных данных для экспериментов - trips
DROP TABLE IF EXISTS default.trips;
CREATE TABLE default.trips (
    trip_id             UInt32,
    pickup_datetime     DateTime,
    dropoff_datetime    DateTime,
    pickup_longitude    Nullable(Float64),
    pickup_latitude     Nullable(Float64),
    dropoff_longitude   Nullable(Float64),
    dropoff_latitude    Nullable(Float64),
    passenger_count     UInt8,
    trip_distance       Float32,
    fare_amount         Float32,
    extra               Float32,
    tip_amount          Float32,
    tolls_amount        Float32,
    total_amount        Float32,
    payment_type        Enum('CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4, 'UNK' = 5),
    pickup_ntaname      LowCardinality(String),
    dropoff_ntaname     LowCardinality(String)
)
ENGINE = MergeTree
PRIMARY KEY (pickup_datetime, dropoff_datetime);

INSERT INTO default.trips
SELECT
    trip_id,
    pickup_datetime,
    dropoff_datetime,
    pickup_longitude,
    pickup_latitude,
    dropoff_longitude,
    dropoff_latitude,
    passenger_count,
    trip_distance,
    fare_amount,
    extra,
    tip_amount,
    tolls_amount,
    total_amount,
    payment_type,
    pickup_ntaname,
    dropoff_ntaname
FROM s3(
    'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_{0..2}.gz',
    'TabSeparatedWithNames'
);

SELECT count(), uniqExact(trip_id) FROM default.trips;  -- оцениваем уникальность trip_id

count()|uniqExact(trip_id)|
-------+------------------+
3000317|           3000317|
```

Эксперименты с бэкапом на уровне кластера оказались неудачны. Предположу, требуется указание макросов шардов в конфиге `clickhouse-backup` и оркестрированный запуск только на одной реплике каждого шарда. Не буду тратить на это время на данном этапе, и далее будет показан бэкап на уровне одного хоста.

Создадим тестовую БД и заполним ее данными

```sql
CREATE DATABASE testbackup3;

CREATE TABLE testbackup3.new_trips
AS default.trips
ENGINE = MergeTree
PRIMARY KEY (pickup_datetime, dropoff_datetime);

-- Заливаем данные
INSERT INTO testbackup3.new_trips SELECT * FROM default.trips;

SELECT count(), uniq(trip_id), uniqExact(trip_id) FROM testbackup3.new_trips;

count()|uniq(trip_id)|uniqExact(trip_id)|
-------+-------------+------------------+
3000317|      3002130|           3000317|
```

Проваливаемся в первый хост CH кластера `sharded_cluster`
```sh
docker compose exec -it clickhouse1 bash
```

Выполним команды `clickhouse-backup` внутри хоста
```sh
clickhouse-backup list

2025-03-23 10:52:40.101 INF pkg/storage/general.go:163 > list_duration=2.130484

clickhouse-backup create --tables='testbackup3.*' bkp01

2025-03-23 10:53:02.578 INF pkg/backup/create.go:181 > done createBackupRBAC size=0B
2025-03-23 10:53:02.624 INF pkg/backup/create.go:351 > done progress=1/1 table=testbackup3.new_trips
2025-03-23 10:53:02.627 INF pkg/backup/create.go:363 > done duration=153ms operation=createBackupLocal version=2.6.8

clickhouse-backup upload bkp01

2025-03-23 10:53:19.414 INF pkg/storage/general.go:163 > list_duration=2.410326
2025-03-23 10:53:19.866 INF pkg/backup/upload.go:184 > done data_size=120.13MiB duration=425ms metadata_size=1.16KiB operation=upload_table progress=1/1 table=testbackup2.new_trips version=2.6.8
2025-03-23 10:53:19.874 INF pkg/backup/upload.go:256 > done backup=bkp01 duration=526ms object_disk_size=0B operation=upload upload_size=120.13MiB version=2.6.8

clickhouse-backup list all

2025-03-23 10:53:32.858 INF pkg/storage/general.go:163 > list_duration=6.88995
bkp01   23/03/2025 10:53:02   local       all:120.01MiB,data:120.01MiB,arch:0B,obj:1.04KiB,meta:0B,rbac:0B,conf:0B          regular
bkp01   23/03/2025 10:53:19   remote      all:120.13MiB,data:120.01MiB,arch:120.13MiB,obj:1.16KiB,meta:0B,rbac:0B,conf:0B   tar, regular

clickhouse-backup delete local bkp01
```

Испортим локальные данные - дропнем БД

```sql
DROP DATABASE testbackup3;

SELECT count(), uniq(trip_id), uniqExact(trip_id) FROM testbackup2.new_trips;

SQL Error [81] [07000]: Code: 81. DB::Exception: Database testbackup3 does not exist. Maybe you meant testbackup?. (UNKNOWN_DATABASE) (version 25.1.8.25 (official build))
```

```sh
clickhouse-backup download bkp01

2025-03-23 11:29:01.366 INF pkg/storage/general.go:163 > list_duration=3.027913
2025-03-23 11:29:01.415 INF pkg/backup/download.go:505 > done table_metadata=testbackup3.new_trips
2025-03-23 11:29:01.491 INF pkg/backup/download.go:220 > done backup_name=bkp01 duration=76ms operation=download_data progress=1/1 size=120.05MiB table=testbackup3.new_trips version=2.6.8
2025-03-23 11:29:01.495 INF pkg/backup/download.go:296 > done backup=bkp01 download_size=120.05MiB duration=152ms object_disk_size=0B operation=download version=2.6.8

# Если мы удаляли объекты без опции NO DELAY и не ждали достаточно времени, восстановление упадет
# https://github.com/Altinity/clickhouse-backup/issues/370
clickhouse-backup restore bkp01

2025-03-23 11:40:39.172 INF pkg/backup/restore.go:920 > done backup=bkp01 duration=47ms operation=restore_schema
2025-03-23 11:40:39.268 INF pkg/backup/restore.go:1461 > download object_disks start table=testbackup3.new_trips
2025-03-23 11:40:39.268 INF pkg/backup/restore.go:1468 > download object_disks finish duration=0s size=0B
2025-03-23 11:40:39.277 INF pkg/backup/restore.go:1421 > done database=testbackup3 duration=14ms operation=restoreDataRegular progress=1/1 table=new_trips
2025-03-23 11:40:39.277 INF pkg/backup/restore.go:1333 > done backup=bkp01 duration=104ms operation=restore_data
2025-03-23 11:40:39.277 INF pkg/backup/restore.go:266 > done duration=192ms operation=restore version=2.6.8
```

Првоерим работоспособность

```sql
SELECT count(), uniq(trip_id), uniqExact(trip_id) FROM testbackup3.new_trips;

count()|uniq(trip_id)|uniqExact(trip_id)|
-------+-------------+------------------+
3000317|      3002130|           3000317|
```

## Настройка Storage Policy

Подключаю конфигурацию к кластеру конфигом `configs/s3-storage.xml`. Обнаруживаю, что при некорректном конфиге или нерабочем s3, кластер CH не запускается. Удивляет, ожидаешь, что БД будет работать независимо от другого сервиса, и говорить об ошибке пользователю, а не молча падать с сообщением в логе в стиле "не могу инициализироваться, у меня имя пользователя на http://... не подходит".

Проверяю работу

```sql
DROP TABLE IF EXISTS trips_s3 NO DELAY;

CREATE TABLE trips_s3
AS default.trips
ENGINE = MergeTree
PRIMARY KEY (pickup_datetime, dropoff_datetime)
SETTINGS storage_policy='minio';

-- Заливаем данные
INSERT INTO trips_s3 SELECT * FROM trips;

-- Проверяем работу таблицы в S3
SELECT count(), uniq(trip_id), uniqExact(trip_id) FROM trips_s3;

count()|uniq(trip_id)|uniqExact(trip_id)|
-------+-------------+------------------+
3000317|      3002130|           3000317|
```

Проверим работу кластера с хранением на S3

```sql
-- Создаем реплицированную таблицу как подложку для шардированной
DROP TABLE IF EXISTS trips_c4sh1rep_source ON CLUSTER c4sh1rep;

-- Таблица-подложка для Distributed engine, с указанием хранения на S3
CREATE TABLE trips_s3_c4sh1rep_source ON CLUSTER c4sh1rep
AS trips 
ENGINE = ReplicatedMergeTree('/clickhouse/shard_{shard_c4sh1rep}/{database}/{table}','{replica_c4sh1rep}')
SETTINGS storage_policy='minio';

CREATE TABLE trips_s3_sharded ON CLUSTER c4sh1rep
AS default.trips
ENGINE = Distributed(c4sh1rep, default, trips_s3_c4sh1rep_source, trip_id);

-- Заливаем данные
INSERT INTO trips_s3_sharded SELECT * FROM trips;

-- Смотрим, нет ли дублей (правильно ли разложилось)
SELECT count(), uniq(trip_id), uniqExact(trip_id) FROM trips_s3_sharded;

count()|uniq(trip_id)|uniqExact(trip_id)|
-------+-------------+------------------+
3000317|      3002130|           3000317|

-- раскладка по шардам
SELECT hostName() AS hostname, shardNum() AS shard_number, count(*) AS cnt FROM trips_s3_sharded AS t GROUP BY 1, 2;

hostname    |shard_number|cnt   |
------------+------------+------+
052c336daded|           1|749220|
03a6e10c8318|           3|750897|
e1c385459abc|           2|750674|
c610f012aa56|           4|749526|
```

Итого - эксперимент с хранилищем кластера на S3 успешен. Эксперимент с бэкапом кластера - требует дополнительного изучения/разработки.
